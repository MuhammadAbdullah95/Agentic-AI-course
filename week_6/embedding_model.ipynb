{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254d0423",
   "metadata": {},
   "source": [
    "# Understanding Embedding Models for RAG\n",
    "\n",
    "## What are Embedding Models?\n",
    "\n",
    "An **embedding model** is a neural network that converts text (words, sentences, or documents) into dense numerical vectors (embeddings). These vectors capture the semantic meaning of the text in a high-dimensional space.\n",
    "\n",
    "### Why Do We Need Embeddings?\n",
    "\n",
    "- **Computers don't understand text naturally** - they work with numbers\n",
    "- **Traditional keyword matching is limited** - it misses semantic relationships\n",
    "- **Embeddings capture meaning** - similar concepts have similar vector representations\n",
    "- **Enable semantic search** - find documents by meaning, not just exact word matches\n",
    "\n",
    "## How Embedding Models Work\n",
    "\n",
    "```\n",
    "Input Text: \"The cat sat on the mat\"\n",
    "           ↓ (Embedding Model)\n",
    "Output Vector: [0.2, -0.5, 0.8, 0.1, ..., -0.3]\n",
    "              (typically 384, 512, or 1536 dimensions)\n",
    "```\n",
    "\n",
    "### Key Properties of Good Embeddings:\n",
    "\n",
    "1. **Semantic Similarity**: Similar meanings → similar vectors\n",
    "2. **Dimensionality**: Usually 100-1536 dimensions\n",
    "3. **Dense Representation**: Every dimension has a meaningful value\n",
    "4. **Cosine Similarity**: Measure how \"close\" two vectors are\n",
    "\n",
    "## Popular Embedding Models\n",
    "\n",
    "### 1. **Sentence Transformers** (Most Common for RAG)\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Popular models:\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')      # Fast, 384 dim\n",
    "model = SentenceTransformer('all-mpnet-base-v2')     # Better quality, 768 dim\n",
    "model = SentenceTransformer('all-distilroberta-v1')  # Good balance, 768 dim\n",
    "```\n",
    "\n",
    "**Pros**: Purpose-built for sentence embeddings, great performance\n",
    "**Cons**: Requires separate installation\n",
    "\n",
    "### 2. **OpenAI Embeddings**\n",
    "```python\n",
    "import openai\n",
    "response = openai.Embedding.create(\n",
    "    model=\"text-embedding-ada-002\",  # 1536 dimensions\n",
    "    input=\"Your text here\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Pros**: High quality, maintained by OpenAI\n",
    "**Cons**: Requires API key, costs money, internet connection\n",
    "\n",
    "### 3. **Hugging Face Models**\n",
    "```python\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "```\n",
    "\n",
    "**Pros**: Many options, free, can run locally\n",
    "**Cons**: More complex setup\n",
    "\n",
    "## ChromaDB and Embedding Models\n",
    "\n",
    "### Default Behavior\n",
    "ChromaDB uses a **default embedding model** if you don't specify one:\n",
    "```python\n",
    "collection = client.create_collection(name=\"my_collection\")\n",
    "# Uses default embedding function automatically\n",
    "```\n",
    "\n",
    "### Custom Embedding Models\n",
    "```python\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Option 1: Sentence Transformers\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Option 2: OpenAI\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=\"your-api-key\",\n",
    "    model_name=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "# Create collection with custom embedding\n",
    "collection = client.create_collection(\n",
    "    name=\"my_collection\",\n",
    "    embedding_function=sentence_transformer_ef\n",
    ")\n",
    "```\n",
    "\n",
    "## Practical Example: Seeing Embeddings in Action\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Example texts\n",
    "texts = [\n",
    "    \"The cat is sleeping on the couch\",\n",
    "    \"A feline is resting on the sofa\",      # Similar meaning\n",
    "    \"The dog is barking loudly\",            # Different meaning\n",
    "    \"Machine learning is a subset of AI\"    # Completely different\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(texts)\n",
    "print(f\"Shape of embeddings: {embeddings.shape}\")  # (4, 384)\n",
    "\n",
    "# Calculate similarities\n",
    "similarities = cosine_similarity(embeddings)\n",
    "print(\"Similarity Matrix:\")\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"{i}: {text[:30]}...\")\n",
    "    \n",
    "for i in range(len(texts)):\n",
    "    for j in range(len(texts)):\n",
    "        print(f\"Text {i} vs Text {j}: {similarities[i][j]:.3f}\")\n",
    "```\n",
    "\n",
    "## Choosing the Right Embedding Model\n",
    "\n",
    "### Consider These Factors:\n",
    "\n",
    "1. **Domain**: General vs specialized (legal, medical, etc.)\n",
    "2. **Performance**: Speed vs accuracy trade-offs\n",
    "3. **Resources**: Memory and computational requirements\n",
    "4. **Cost**: Free vs paid models\n",
    "5. **Language**: Multi-language support needed?\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "| Use Case | Recommended Model | Why |\n",
    "|----------|------------------|-----|\n",
    "| **Learning/Prototyping** | `all-MiniLM-L6-v2` | Fast, lightweight, good quality |\n",
    "| **Production RAG** | `all-mpnet-base-v2` | Better quality, still manageable size |\n",
    "| **High-end Applications** | OpenAI `text-embedding-ada-002` | State-of-the-art quality |\n",
    "| **Multilingual** | `paraphrase-multilingual-MiniLM-L12-v2` | Supports 50+ languages |\n",
    "\n",
    "## Common Pitfalls to Avoid\n",
    "\n",
    "1. **Mixing embedding models**: Always use the same model for indexing and querying\n",
    "2. **Ignoring context length**: Models have token limits (usually 512 tokens)\n",
    "3. **Not considering domain**: Generic models might not work well for specialized content\n",
    "4. **Overlooking preprocessing**: Text cleaning can significantly impact quality\n",
    "\n",
    "## Testing Your Understanding\n",
    "\n",
    "Try this exercise:\n",
    "1. Take the sentence \"The weather is beautiful today\"\n",
    "2. Generate embeddings using different models\n",
    "3. Compare with \"Today has lovely weather\" and \"The computer is broken\"\n",
    "4. Observe which pairs have higher similarity scores\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand embedding models, we can:\n",
    "1. Set up ChromaDB with a specific embedding model\n",
    "2. Add our dummy documents to the collection\n",
    "3. Perform semantic searches\n",
    "4. Build a complete RAG pipeline\n",
    "\n",
    "Remember: **The quality of your RAG system heavily depends on choosing the right embedding model for your use case!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "672fe70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03bf503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc2aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentEmbedding(\n",
      "  values=[\n",
      "    -0.022374554,\n",
      "    -0.004560777,\n",
      "    0.013309286,\n",
      "    -0.0545072,\n",
      "    -0.02090443,\n",
      "    <... 3067 more items ...>,\n",
      "  ]\n",
      ")]\n",
      "3072\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=api_key)\n",
    "\n",
    "result = client.models.embed_content(\n",
    "        model=\"gemini-embedding-001\", # 768\n",
    "        contents=\"What is the meaning of life?\")\n",
    "\n",
    "print(result.embeddings)\n",
    "embedding_vector = result.embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0546d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072\n"
     ]
    }
   ],
   "source": [
    "print(len(embedding_vector.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd9883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-ai-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
